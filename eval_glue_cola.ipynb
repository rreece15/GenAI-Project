{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f42cf13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-26 22:09:34.663082: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-26 22:09:34.669690: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1745723374.677881 1395646 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1745723374.680269 1395646 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-26 22:09:34.689760: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from datasets import load_from_disk\n",
    "from eval.cola import evaluate, generate_answers_batch\n",
    "import torch\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02fb4103",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b76645758cbd4df9a39415ae7fdfb2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the model and tokenizer\n",
    "MODEL_ID = \"mistralai/Mistral-7B-v0.1\"\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID,\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d0ff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "OptimizedModule(\n",
       "  (_orig_mod): PeftModelForCausalLM(\n",
       "    (base_model): LoraModel(\n",
       "      (model): MistralForCausalLM(\n",
       "        (model): MistralModel(\n",
       "          (embed_tokens): Embedding(32000, 4096)\n",
       "          (layers): ModuleList(\n",
       "            (0-31): 32 x MistralDecoderLayer(\n",
       "              (self_attn): MistralAttention(\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=4096, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=4096, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "              )\n",
       "              (mlp): MistralMLP(\n",
       "                (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "                (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "                (act_fn): SiLU()\n",
       "              )\n",
       "              (input_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "              (post_attention_layernorm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "            )\n",
       "          )\n",
       "          (norm): MistralRMSNorm((4096,), eps=1e-05)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add adapter, if not then base model\n",
    "# model = base_model\n",
    "\n",
    "# load pretrained adapter\n",
    "# peft_model_gsm8k_id = \"predibase/gsm8k\"\n",
    "# peft_model_magicoder_id = \"predibase/magicoder\"\n",
    "# peft_model_gluecola_id = \"predibase/glue_cola\"\n",
    "# peft_model_hellaswag_id = \"predibase/hellaswag\"\n",
    "\n",
    "# adapter_name_gsm8k = \"gsm8k\"\n",
    "# adapter_name_magicoder = \"magicoder\"\n",
    "# adapter_name_gluecola = \"glue_cola\"\n",
    "# adapter_name_hellaswag = \"hellaswag\"\n",
    "\n",
    "# model = PeftModel.from_pretrained( # not loading from local\n",
    "#     base_model,\n",
    "#     peft_model_hellaswag_id,\n",
    "#     adapter_name=adapter_name_hellaswag, # You can name the first adapter here\n",
    "#     device_map=\"auto\", # Apply device mapping here if needed\n",
    "#     # low_cpu_mem_usage=True,\n",
    "#     # offload_folder='offload/'\n",
    "# )\n",
    "\n",
    "# load merged adapter\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    \"./weights/element_add/gsm8k_magicoder_hellaswag/gsm8k_magicoder_hellaswag\",\n",
    "    device_map='auto'\n",
    "    )\n",
    "model = model.half()        # convert to half precision (if your GPU supports it)\n",
    "model = torch.compile(model) \n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f70e37bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Tara hasn't cured themselves.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"data/blimp_adjunct_anaphor_refined\")\n",
    "dataset['sentence'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c4c602bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/6000 [00:00<?, ?it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 2/6000 [00:00<06:50, 14.60it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 4/6000 [00:00<06:29, 15.39it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the following sentence grammatically correct?\n",
      "sentence: \"tara hasn't cured themselves.\"\n",
      "answer:\n",
      "0\n",
      "is the following sentence grammatically correct?\n",
      "sentence: \"this government alarms themselves.\"\n",
      "answer:\n",
      "0\n",
      "is the following sentence grammatically correct?\n",
      "sentence: \"who had every actress hidden before scaring jessica?\"\n",
      "answer:\n",
      "1\n",
      "is the following sentence grammatically correct?\n",
      "sentence: \"barbara isn't escaping from himself.\"\n",
      "answer:\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 6/6000 [00:00<06:22, 15.65it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 8/6000 [00:00<06:18, 15.82it/s]Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the following sentence grammatically correct?\n",
      "sentence: \"edward had distracted themselves.\"\n",
      "answer:\n",
      "0\n",
      "is the following sentence grammatically correct?\n",
      "sentence: \"the lutherans found itself.\"\n",
      "answer:\n",
      "0\n",
      "is the following sentence grammatically correct?\n",
      "sentence: \"patricia scared himself.\"\n",
      "answer:\n",
      "0\n",
      "is the following sentence grammatically correct?\n",
      "sentence: \"kevin doesn't care for himself.\"\n",
      "answer:\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "  0%|          | 9/6000 [00:00<07:03, 14.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "is the following sentence grammatically correct?\n",
      "sentence: \"who can this cart worry after upsetting waiters?\"\n",
      "answer:\n",
      "1\n",
      "is the following sentence grammatically correct?\n",
      "sentence: \"what was debra breaking after cleaning some forks?\"\n",
      "answer:\n",
      "1\n",
      "Accuracy (CoLA): 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "prompt_template = (\n",
    "    \"Is the following sentence grammatically correct?\\n\"\n",
    "    \"Sentence: \\\"{sentence}\\\"\\n\"\n",
    "    \"Answer:\"\n",
    ")\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "for example in tqdm(dataset):\n",
    "    sentence = example[\"sentence\"]\n",
    "    label = example[\"label\"]\n",
    "    \n",
    "    prompt = prompt_template.format(sentence=sentence)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(**inputs, max_new_tokens=3)\n",
    "    \n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).lower()\n",
    "    print(response)\n",
    "    print(label)\n",
    "    # Crude but effective classification\n",
    "    if \"yes\" in response:\n",
    "        pred = 1\n",
    "    elif \"no\" in response:\n",
    "        pred = 0\n",
    "    else:\n",
    "        pred = 0  # default fallback\n",
    "\n",
    "    all_preds.append(pred)\n",
    "    all_labels.append(label)\n",
    "    if len(all_preds) == 10:\n",
    "        break\n",
    "    \n",
    "accuracy = accuracy_score(all_labels, all_preds)\n",
    "print(f\"Accuracy (CoLA): {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77a84d3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/188 [00:00<?, ?it/s]Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "100%|██████████| 188/188 [00:48<00:00,  3.84it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    dataset=dataset,\n",
    "    batch_size=32\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
