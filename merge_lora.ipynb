{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-06 17:14:49.840368: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-06 17:14:49.842637: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2025-04-06 17:14:49.846587: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1743977689.854075 2936357 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1743977689.856285 2936357 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-06 17:14:49.864650: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from huggingface_hub import login\n",
    "from peft import PeftModel\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "peft_model_gsm8k_id = \"predibase/gsm8k\"\n",
    "peft_model_magicoder_id = \"predibase/magicoder\"\n",
    "adapter_name_gsm8k = \"gsm8k\"\n",
    "adapter_name_magicoder = \"magicoder\"\n",
    "merged_adapter_name = \"gsm8k_magicoder_avg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    compute_dtype = torch.bfloat16 # Or torch.float16 depending on your GPU\n",
    "else:\n",
    "    compute_dtype = torch.float32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load model and Adaptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aaeff16784c34f9b9377770764aa869a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=compute_dtype,\n",
    "    device_map=\"auto\" # Automatically distributes across GPUs if available/needed\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: BNB_CUDA_VERSION=118 environment variable detected; loading libbitsandbytes_cuda118.so.\n",
      "This can be used to load a bitsandbytes version that is different from the PyTorch CUDA version.\n",
      "If this was unintended set the BNB_CUDA_VERSION variable to an empty string: export BNB_CUDA_VERSION=\n",
      "If you use the manual override make sure the right libcudart.so is in your LD_LIBRARY_PATH\n",
      "For example by adding the following to your .bashrc: export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:<path_to_cuda_dir/lib64\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    peft_model_gsm8k_id,\n",
    "    adapter_name=adapter_name_gsm8k, # You can name the first adapter here\n",
    "    device_map=\"auto\" # Apply device mapping here if needed\n",
    ")\n",
    "model.load_adapter(peft_model_magicoder_id, adapter_name=adapter_name_magicoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add_weighted_adapter(\n",
    "    adapters=[adapter_name_gsm8k, adapter_name_magicoder],\n",
    "    weights=[0.5, 0.5],\n",
    "    adapter_name=merged_adapter_name,\n",
    "    combination_type=\"linear\" # 'linear' is the default for weighted sum\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving the merged adapter 'gsm8k_magicoder_avg' to weights/gsm8k_magicoder_avg...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('weights/gsm8k_magicoder_avg/tokenizer_config.json',\n",
       " 'weights/gsm8k_magicoder_avg/special_tokens_map.json',\n",
       " 'weights/gsm8k_magicoder_avg/tokenizer.model',\n",
       " 'weights/gsm8k_magicoder_avg/added_tokens.json',\n",
       " 'weights/gsm8k_magicoder_avg/tokenizer.json')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.set_adapter(merged_adapter_name)\n",
    "\n",
    "save_directory = f\"weights/{merged_adapter_name}\"\n",
    "print(f\"\\nSaving the merged adapter '{merged_adapter_name}' to {save_directory}...\")\n",
    "model.save_pretrained(save_directory, selected_adapters=[merged_adapter_name])\n",
    "tokenizer.save_pretrained(save_directory) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing the merged model ---\n",
      "Current active adapter: gsm8k_magicoder_avg\n",
      "\n",
      "Prompt: What is 5 * 8 + 3?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/archy1/miniforge3/envs/gen-models/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:628: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.7` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Output:\n",
      "What is 5 * 8 + 3?\n",
      "\n",
      "##### Answer: 41\n",
      "##### 5 * 8 = 40\n",
      "##### 40 + 3 = 43\n",
      "##### 43 - 3 = 40\n",
      "\n",
      "\n",
      "Prompt: def fibonacci(n):\n",
      "Generated Output:\n",
      "def fibonacci(n):\n",
      "    if n == 0:\n",
      "        return 0\n",
      "    if n == 1:\n",
      "        return 1\n",
      "    return fibonacci(n-1) + fibonacci(n-2)\n",
      "\n",
      "n =\n",
      "\n",
      "--- Example of loading the saved merged adapter later ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Testing the merged model ---\")\n",
    "# Make sure the merged adapter is active (we did this in step 5)\n",
    "print(f\"Current active adapter: {model.active_adapter}\") # Verify it's the merged one\n",
    "\n",
    "prompt_gsm8k = \"What is 5 * 8 + 3?\" # Example GSM8K style\n",
    "prompt_magicoder = \"def fibonacci(n):\" # Example Magicoder style\n",
    "\n",
    "# Ensure tokenizer pads correctly if needed for batching (though here we do one by one)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "for prompt in [prompt_gsm8k, prompt_magicoder]:\n",
    "    print(f\"\\nPrompt: {prompt}\")\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(model.device) # Use model.device\n",
    "    with torch.no_grad(): # Ensure no gradients are calculated during inference\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,\n",
    "            temperature=0.7,\n",
    "            pad_token_id=tokenizer.pad_token_id # Important for generation\n",
    "            )\n",
    "    decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    print(f\"Generated Output:\\n{decoded_output}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen-models",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
